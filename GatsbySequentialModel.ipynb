{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJfPN-BJvLb7",
        "outputId": "55ee660c-6148-4144-87af-31d53fa6705d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Project Gutenberg eBook of The Great Gatsby\n",
            "    \n",
            "This ebook is for the use of anyone anywhere in the United States and\n",
            "most other parts of the world at no cost and with almost no restrictions\n",
            "what\n",
            "['the', 'project', 'gutenberg', 'ebook', 'of', 'the', 'great', 'gatsby', 'this', 'ebook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'in', 'the', 'united', 'states', 'and', 'most', 'other', 'parts', 'of', 'the', 'world', 'at', 'no', 'cost', 'and', 'with', 'almost', 'no', 'restrictions', 'whatsoever', 'you', 'may', 'copy', 'it', 'give', 'it', 'away', 'or', 'reuse', 'it', 'under', 'the', 'terms', 'of', 'the', 'project', 'gutenberg', 'license', 'included', 'with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergorg', 'if', 'you', 'are', 'not', 'located', 'in', 'the', 'united', 'states', 'you', 'will', 'have', 'to', 'check', 'the', 'laws', 'of', 'the', 'country', 'where', 'you', 'are', 'located', 'before', 'using', 'this', 'ebook', 'title', 'the', 'great', 'gatsby', 'author', 'f', 'scott', 'fitzgerald', 'release', 'date', 'january', 'ebook', 'language', 'english', 'start', 'of', 'the', 'project', 'gutenberg', 'ebook', 'the', 'great', 'gatsby', 'the', 'great', 'gatsby', 'by', 'f', 'scott', 'fitzgerald', 'table', 'of', 'contents', 'i', 'ii', 'iii', 'iv', 'v', 'vi', 'vii', 'viii', 'ix', 'once', 'again', 'to', 'zelda', 'then', 'wear', 'the', 'gold', 'hat', 'if', 'that', 'will', 'move', 'her', 'if', 'you', 'can', 'bounce', 'high', 'bounce', 'for', 'her', 'too', 'till', 'she', 'cry', 'goldhatted', 'highbouncing', 'lover', 'i', 'must', 'have', 'thomas', 'parke', 'i', 'in', 'my', 'younger', 'and', 'more', 'vulnerable', 'years', 'my', 'father', 'gave', 'me', 'some', 'advice', 'that', 'been', 'turning', 'over', 'in', 'my', 'mind', 'ever', 'since', 'you', 'feel', 'like', 'criticizing', 'he', 'told', 'me', 'remember', 'that', 'all', 'the']\n",
            "Total Tokens: 46804\n",
            "Unique Tokens: 5996\n",
            "Total Sequences: 46703\n"
          ]
        }
      ],
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import Input\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import string\n",
        "\n",
        "# changeable params\n",
        "my_file = \"TheGreatGatsby.txt\"\n",
        "seq_len = 100\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        " # open the file as read only\n",
        " file = open(filename, 'r')\n",
        " # read all text\n",
        " text = file.read()\n",
        " # close the file\n",
        " file.close()\n",
        " return text\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        " # replace '--' with a space ' '\n",
        " doc = doc.replace('--', ' ')\n",
        " # split into tokens by white space\n",
        " tokens = doc.split()\n",
        " # remove punctuation from each token\n",
        " table = str.maketrans('', '', string.punctuation)\n",
        " tokens = [w.translate(table) for w in tokens]\n",
        " # remove remaining tokens that are not alphabetic\n",
        " tokens = [word for word in tokens if word.isalpha()]\n",
        " # make lower case\n",
        " tokens = [word.lower() for word in tokens]\n",
        " return tokens\n",
        "\n",
        "# save tokens to file, one dialog per line\n",
        "def save_doc(lines, filename):\n",
        " data = '\\n'.join(lines)\n",
        " file = open(filename, 'w')\n",
        " file.write(data)\n",
        " file.close()\n",
        "\n",
        "# load document\n",
        "doc = load_doc(my_file)\n",
        "print(doc[:200])\n",
        "\n",
        "# clean document\n",
        "tokens = clean_doc(doc)\n",
        "print(tokens[:200])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))\n",
        "\n",
        "# organize into sequences of tokens\n",
        "length = seq_len + 1\n",
        "sequences = list()\n",
        "for i in range(length, len(tokens)):\n",
        " # select sequence of tokens\n",
        " seq = tokens[i-length:i]\n",
        " # convert into a line\n",
        " line = ' '.join(seq)\n",
        " # store\n",
        " sequences.append(line)\n",
        "print('Total Sequences: %d' % len(sequences))\n",
        "\n",
        "# save sequences to file\n",
        "out_filename = my_file[:-4] + '_seq.txt'\n",
        "save_doc(sequences, out_filename)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        " # open the file as read only\n",
        " file = open(filename, 'r')\n",
        " # read all text\n",
        " text = file.read()\n",
        " # close the file\n",
        " file.close()\n",
        " return text\n",
        "\n",
        "# load\n",
        "doc = load_doc(out_filename)\n",
        "lines = doc.split('\\n')\n",
        "\n",
        "# integer encode sequences of words\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(lines)\n",
        "sequences = tokenizer.texts_to_sequences(lines)"
      ],
      "metadata": {
        "id": "HnNk2XcLv8Ex"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "\n",
        "# separate into input and output\n",
        "sequences = np.array(sequences)\n",
        "sequences.shape\n",
        "X, y = sequences[:,:-1], sequences[:,-1]\n",
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "seq_length = X.shape[1]\n",
        "\n",
        "p_train = 0.8\n",
        "\n",
        "n_train = int(X.shape[0]//(1/p_train))\n",
        "X_train = X[0:n_train]\n",
        "y_train = y[0:n_train]\n",
        "X_test = X[n_train:]\n",
        "y_test = y[n_train:]\n",
        "\n"
      ],
      "metadata": {
        "id": "vVJkSix4zpzY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 25, input_length=seq_length))\n",
        "model.add(LSTM(150, return_sequences=True))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(150, activation='relu'))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "print(model.summary())\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "# fit model\n",
        "model.fit(X_train, y_train, batch_size=128, epochs=200)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epI1sV3pzwqO",
        "outputId": "4f291c7b-09ff-45ea-eb28-f5e1267f46f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 25)           149925    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 100, 150)          105600    \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 150)               180600    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 150)               22650     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5997)              905547    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1364322 (5.20 MB)\n",
            "Trainable params: 1364322 (5.20 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/200\n",
            "292/292 [==============================] - 39s 107ms/step - loss: 6.7776 - accuracy: 0.0535\n",
            "Epoch 2/200\n",
            "292/292 [==============================] - 15s 49ms/step - loss: 6.3457 - accuracy: 0.0538\n",
            "Epoch 3/200\n",
            "292/292 [==============================] - 10s 34ms/step - loss: 6.1657 - accuracy: 0.0666\n",
            "Epoch 4/200\n",
            "292/292 [==============================] - 9s 30ms/step - loss: 6.0191 - accuracy: 0.0764\n",
            "Epoch 5/200\n",
            "292/292 [==============================] - 8s 27ms/step - loss: 5.9006 - accuracy: 0.0843\n",
            "Epoch 6/200\n",
            "292/292 [==============================] - 7s 24ms/step - loss: 5.7988 - accuracy: 0.0919\n",
            "Epoch 7/200\n",
            "292/292 [==============================] - 8s 26ms/step - loss: 5.6976 - accuracy: 0.0973\n",
            "Epoch 8/200\n",
            "292/292 [==============================] - 6s 21ms/step - loss: 5.6019 - accuracy: 0.1013\n",
            "Epoch 9/200\n",
            "292/292 [==============================] - 6s 22ms/step - loss: 5.5117 - accuracy: 0.1056\n",
            "Epoch 10/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 5.4291 - accuracy: 0.1094\n",
            "Epoch 11/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 5.3507 - accuracy: 0.1140\n",
            "Epoch 12/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 5.2737 - accuracy: 0.1172\n",
            "Epoch 13/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 5.1998 - accuracy: 0.1203\n",
            "Epoch 14/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 5.1229 - accuracy: 0.1259\n",
            "Epoch 15/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 5.0550 - accuracy: 0.1280\n",
            "Epoch 16/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 4.9789 - accuracy: 0.1313\n",
            "Epoch 17/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 4.9048 - accuracy: 0.1343\n",
            "Epoch 18/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 4.8329 - accuracy: 0.1369\n",
            "Epoch 19/200\n",
            "292/292 [==============================] - 6s 21ms/step - loss: 4.7678 - accuracy: 0.1384\n",
            "Epoch 20/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 4.6986 - accuracy: 0.1397\n",
            "Epoch 21/200\n",
            "292/292 [==============================] - 6s 21ms/step - loss: 4.6329 - accuracy: 0.1432\n",
            "Epoch 22/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 4.5712 - accuracy: 0.1455\n",
            "Epoch 23/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 4.5092 - accuracy: 0.1492\n",
            "Epoch 24/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 4.4467 - accuracy: 0.1529\n",
            "Epoch 25/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 4.3903 - accuracy: 0.1551\n",
            "Epoch 26/200\n",
            "292/292 [==============================] - 6s 21ms/step - loss: 4.3324 - accuracy: 0.1587\n",
            "Epoch 27/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 4.2782 - accuracy: 0.1609\n",
            "Epoch 28/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 4.2260 - accuracy: 0.1659\n",
            "Epoch 29/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 4.1938 - accuracy: 0.1669\n",
            "Epoch 30/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 4.5512 - accuracy: 0.1487\n",
            "Epoch 31/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 4.2663 - accuracy: 0.1633\n",
            "Epoch 32/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 4.2140 - accuracy: 0.1670\n",
            "Epoch 33/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 4.0780 - accuracy: 0.1768\n",
            "Epoch 34/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 4.0211 - accuracy: 0.1823\n",
            "Epoch 35/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 3.9665 - accuracy: 0.1889\n",
            "Epoch 36/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 3.9179 - accuracy: 0.1946\n",
            "Epoch 37/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 3.8818 - accuracy: 0.1984\n",
            "Epoch 38/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 3.8354 - accuracy: 0.2036\n",
            "Epoch 39/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 3.7891 - accuracy: 0.2088\n",
            "Epoch 40/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 3.7437 - accuracy: 0.2158\n",
            "Epoch 41/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 3.7142 - accuracy: 0.2170\n",
            "Epoch 42/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 3.7104 - accuracy: 0.2190\n",
            "Epoch 43/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 3.6668 - accuracy: 0.2251\n",
            "Epoch 44/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 3.6300 - accuracy: 0.2302\n",
            "Epoch 45/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 3.5898 - accuracy: 0.2352\n",
            "Epoch 46/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 3.5564 - accuracy: 0.2400\n",
            "Epoch 47/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 3.5227 - accuracy: 0.2466\n",
            "Epoch 48/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 3.4858 - accuracy: 0.2514\n",
            "Epoch 49/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 3.4560 - accuracy: 0.2555\n",
            "Epoch 50/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 3.4234 - accuracy: 0.2620\n",
            "Epoch 51/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 3.3936 - accuracy: 0.2647\n",
            "Epoch 52/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 3.3612 - accuracy: 0.2714\n",
            "Epoch 53/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 3.3352 - accuracy: 0.2744\n",
            "Epoch 54/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 3.3089 - accuracy: 0.2777\n",
            "Epoch 55/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 3.2778 - accuracy: 0.2836\n",
            "Epoch 56/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 3.2520 - accuracy: 0.2858\n",
            "Epoch 57/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 3.2237 - accuracy: 0.2916\n",
            "Epoch 58/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 3.1949 - accuracy: 0.2949\n",
            "Epoch 59/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 3.1717 - accuracy: 0.3005\n",
            "Epoch 60/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 3.1412 - accuracy: 0.3035\n",
            "Epoch 61/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 3.1125 - accuracy: 0.3104\n",
            "Epoch 62/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 3.0854 - accuracy: 0.3140\n",
            "Epoch 63/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 3.0625 - accuracy: 0.3151\n",
            "Epoch 64/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 3.0366 - accuracy: 0.3225\n",
            "Epoch 65/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 3.0100 - accuracy: 0.3229\n",
            "Epoch 66/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.9804 - accuracy: 0.3322\n",
            "Epoch 67/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.9620 - accuracy: 0.3321\n",
            "Epoch 68/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.9410 - accuracy: 0.3357\n",
            "Epoch 69/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 2.9085 - accuracy: 0.3426\n",
            "Epoch 70/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.8827 - accuracy: 0.3487\n",
            "Epoch 71/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.8583 - accuracy: 0.3516\n",
            "Epoch 72/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.8362 - accuracy: 0.3573\n",
            "Epoch 73/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.8170 - accuracy: 0.3587\n",
            "Epoch 74/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 2.7888 - accuracy: 0.3635\n",
            "Epoch 75/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 2.7648 - accuracy: 0.3682\n",
            "Epoch 76/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.7524 - accuracy: 0.3713\n",
            "Epoch 77/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 2.7210 - accuracy: 0.3753\n",
            "Epoch 78/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.6987 - accuracy: 0.3809\n",
            "Epoch 79/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.6761 - accuracy: 0.3834\n",
            "Epoch 80/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.6584 - accuracy: 0.3878\n",
            "Epoch 81/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.6334 - accuracy: 0.3909\n",
            "Epoch 82/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.6137 - accuracy: 0.3944\n",
            "Epoch 83/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.5901 - accuracy: 0.3996\n",
            "Epoch 84/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.5694 - accuracy: 0.4055\n",
            "Epoch 85/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.5475 - accuracy: 0.4068\n",
            "Epoch 86/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.5212 - accuracy: 0.4128\n",
            "Epoch 87/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.5023 - accuracy: 0.4174\n",
            "Epoch 88/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.4843 - accuracy: 0.4206\n",
            "Epoch 89/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.4665 - accuracy: 0.4226\n",
            "Epoch 90/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.4467 - accuracy: 0.4268\n",
            "Epoch 91/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.4252 - accuracy: 0.4308\n",
            "Epoch 92/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.4023 - accuracy: 0.4346\n",
            "Epoch 93/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.3872 - accuracy: 0.4365\n",
            "Epoch 94/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.3661 - accuracy: 0.4404\n",
            "Epoch 95/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.3408 - accuracy: 0.4479\n",
            "Epoch 96/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.3302 - accuracy: 0.4495\n",
            "Epoch 97/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.3036 - accuracy: 0.4533\n",
            "Epoch 98/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.2865 - accuracy: 0.4575\n",
            "Epoch 99/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.2681 - accuracy: 0.4586\n",
            "Epoch 100/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.2543 - accuracy: 0.4659\n",
            "Epoch 101/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.2282 - accuracy: 0.4695\n",
            "Epoch 102/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.2145 - accuracy: 0.4703\n",
            "Epoch 103/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.1928 - accuracy: 0.4755\n",
            "Epoch 104/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 2.1782 - accuracy: 0.4796\n",
            "Epoch 105/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.1575 - accuracy: 0.4830\n",
            "Epoch 106/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.1384 - accuracy: 0.4856\n",
            "Epoch 107/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.1211 - accuracy: 0.4926\n",
            "Epoch 108/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.1035 - accuracy: 0.4941\n",
            "Epoch 109/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.0898 - accuracy: 0.4969\n",
            "Epoch 110/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.0715 - accuracy: 0.4995\n",
            "Epoch 111/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.0587 - accuracy: 0.5028\n",
            "Epoch 112/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 2.0351 - accuracy: 0.5090\n",
            "Epoch 113/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 2.0171 - accuracy: 0.5127\n",
            "Epoch 114/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 2.0039 - accuracy: 0.5153\n",
            "Epoch 115/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.9886 - accuracy: 0.5181\n",
            "Epoch 116/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.9701 - accuracy: 0.5221\n",
            "Epoch 117/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.9545 - accuracy: 0.5255\n",
            "Epoch 118/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.9371 - accuracy: 0.5311\n",
            "Epoch 119/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.9218 - accuracy: 0.5306\n",
            "Epoch 120/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.9024 - accuracy: 0.5366\n",
            "Epoch 121/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.8863 - accuracy: 0.5428\n",
            "Epoch 122/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.8706 - accuracy: 0.5441\n",
            "Epoch 123/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.8618 - accuracy: 0.5447\n",
            "Epoch 124/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.8450 - accuracy: 0.5494\n",
            "Epoch 125/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.8176 - accuracy: 0.5548\n",
            "Epoch 126/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.8086 - accuracy: 0.5563\n",
            "Epoch 127/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.7936 - accuracy: 0.5583\n",
            "Epoch 128/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.7748 - accuracy: 0.5645\n",
            "Epoch 129/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.7551 - accuracy: 0.5682\n",
            "Epoch 130/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.7504 - accuracy: 0.5677\n",
            "Epoch 131/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.7336 - accuracy: 0.5715\n",
            "Epoch 132/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.7242 - accuracy: 0.5764\n",
            "Epoch 133/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.6991 - accuracy: 0.5788\n",
            "Epoch 134/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.6859 - accuracy: 0.5834\n",
            "Epoch 135/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.6723 - accuracy: 0.5859\n",
            "Epoch 136/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.6568 - accuracy: 0.5881\n",
            "Epoch 137/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.6403 - accuracy: 0.5934\n",
            "Epoch 138/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 1.6290 - accuracy: 0.5961\n",
            "Epoch 139/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.6127 - accuracy: 0.5992\n",
            "Epoch 140/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.5977 - accuracy: 0.6017\n",
            "Epoch 141/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.5831 - accuracy: 0.6057\n",
            "Epoch 142/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.5653 - accuracy: 0.6109\n",
            "Epoch 143/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.5578 - accuracy: 0.6108\n",
            "Epoch 144/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.5410 - accuracy: 0.6143\n",
            "Epoch 145/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.5342 - accuracy: 0.6154\n",
            "Epoch 146/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.5204 - accuracy: 0.6200\n",
            "Epoch 147/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.5016 - accuracy: 0.6234\n",
            "Epoch 148/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.4765 - accuracy: 0.6313\n",
            "Epoch 149/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.4660 - accuracy: 0.6335\n",
            "Epoch 150/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.4627 - accuracy: 0.6347\n",
            "Epoch 151/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.4827 - accuracy: 0.6353\n",
            "Epoch 152/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 1.4537 - accuracy: 0.6359\n",
            "Epoch 153/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.4256 - accuracy: 0.6436\n",
            "Epoch 154/200\n",
            "292/292 [==============================] - 6s 20ms/step - loss: 1.4200 - accuracy: 0.6458\n",
            "Epoch 155/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.4049 - accuracy: 0.6474\n",
            "Epoch 156/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 1.4008 - accuracy: 0.6484\n",
            "Epoch 157/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 1.3777 - accuracy: 0.6528\n",
            "Epoch 158/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 1.3701 - accuracy: 0.6567\n",
            "Epoch 159/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.3713 - accuracy: 0.6575\n",
            "Epoch 160/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 1.3524 - accuracy: 0.6582\n",
            "Epoch 161/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.3369 - accuracy: 0.6629\n",
            "Epoch 162/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 1.3071 - accuracy: 0.6698\n",
            "Epoch 163/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 1.2776 - accuracy: 0.6756\n",
            "Epoch 164/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.2678 - accuracy: 0.6791\n",
            "Epoch 165/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 1.2595 - accuracy: 0.6807\n",
            "Epoch 166/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 1.2534 - accuracy: 0.6786\n",
            "Epoch 167/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 1.2285 - accuracy: 0.6876\n",
            "Epoch 168/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 1.2100 - accuracy: 0.6915\n",
            "Epoch 169/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.1947 - accuracy: 0.6968\n",
            "Epoch 170/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 1.1914 - accuracy: 0.6965\n",
            "Epoch 171/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.1903 - accuracy: 0.6984\n",
            "Epoch 172/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.1711 - accuracy: 0.7029\n",
            "Epoch 173/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.1544 - accuracy: 0.7050\n",
            "Epoch 174/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.1348 - accuracy: 0.7114\n",
            "Epoch 175/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 1.1257 - accuracy: 0.7126\n",
            "Epoch 176/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.1152 - accuracy: 0.7134\n",
            "Epoch 177/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.0974 - accuracy: 0.7197\n",
            "Epoch 178/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.1326 - accuracy: 0.7155\n",
            "Epoch 179/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.1037 - accuracy: 0.7199\n",
            "Epoch 180/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 1.0770 - accuracy: 0.7238\n",
            "Epoch 181/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.0539 - accuracy: 0.7315\n",
            "Epoch 182/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.1009 - accuracy: 0.7253\n",
            "Epoch 183/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.0688 - accuracy: 0.7316\n",
            "Epoch 184/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 1.0381 - accuracy: 0.7346\n",
            "Epoch 185/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 1.0525 - accuracy: 0.7351\n",
            "Epoch 186/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 1.0036 - accuracy: 0.7448\n",
            "Epoch 187/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 0.9918 - accuracy: 0.7462\n",
            "Epoch 188/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 0.9880 - accuracy: 0.7486\n",
            "Epoch 189/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 0.9699 - accuracy: 0.7520\n",
            "Epoch 190/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 1.0498 - accuracy: 0.7423\n",
            "Epoch 191/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.3460 - accuracy: 0.7017\n",
            "Epoch 192/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.2204 - accuracy: 0.7140\n",
            "Epoch 193/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.3230 - accuracy: 0.6975\n",
            "Epoch 194/200\n",
            "292/292 [==============================] - 5s 19ms/step - loss: 1.1964 - accuracy: 0.7171\n",
            "Epoch 195/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 1.0330 - accuracy: 0.7419\n",
            "Epoch 196/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 0.9879 - accuracy: 0.7500\n",
            "Epoch 197/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 0.9807 - accuracy: 0.7568\n",
            "Epoch 198/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 1.0395 - accuracy: 0.7500\n",
            "Epoch 199/200\n",
            "292/292 [==============================] - 6s 19ms/step - loss: 0.9994 - accuracy: 0.7557\n",
            "Epoch 200/200\n",
            "292/292 [==============================] - 5s 18ms/step - loss: 0.9975 - accuracy: 0.7580\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7a4f220aeec0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
        "  result = list()\n",
        "  in_text = seed_text\n",
        "# generate a fixed number of words\n",
        "  for _ in range(n_words):\n",
        "# encode the text as integer\n",
        "    encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
        "# truncate sequences to a fixed length\n",
        "    encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
        "# predict probabilities for each word\n",
        "    yhat = np.argmax(model.predict(encoded, verbose=0), axis=-1)\n",
        "    print(yhat)\n",
        "# map predicted word index to word\n",
        "    out_word = ''\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "      if index == yhat:\n",
        "        out_word = word\n",
        "        break\n",
        "# append to input\n",
        "    in_text += ' ' + out_word\n",
        "    result.append(out_word)\n",
        "  return ' '.join(result)\n",
        "\n",
        "# load cleaned text sequences\n",
        "in_filename = 'TheGreatGatsby_seq.txt'\n",
        "doc = load_doc(in_filename)\n",
        "lines = doc.split('\\n')\n",
        "seq_length = len(lines[0].split()) - 1\n",
        "\n",
        "\n",
        "# select a seed text\n",
        "#seed_text = lines[randint(0,len(lines))]\n",
        "#print(seed_text + '\\n')\n",
        "\n",
        "# generate new text\n",
        "#generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
        "#print(generated)\n",
        "\n",
        "\n",
        "# Generate and assess 10 sequences\n",
        "for _ in range(10):\n",
        "    seed_text = lines[randint(0, len(lines))]\n",
        "    generated = generate_seq(model, tokenizer, seq_length, seed_text, 50)\n",
        "    #print(f\"Seed Text: {seed_text}\\nGenerated Sequence: {generated}\\n\")\n",
        "    print(f\"\\nGenerated Sequence: {generated}\\n\")\n",
        "    #print(\"Basic Assessment: [Your assessment here]\\n\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifJfMQbZEL9p",
        "outputId": "08b0ba25-ca41-4983-f7e6-3c21dc707dc8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[38]\n",
            "[141]\n",
            "[45]\n",
            "[2527]\n",
            "[4730]\n",
            "[294]\n",
            "[20]\n",
            "[31]\n",
            "[4564]\n",
            "[1337]\n",
            "[4565]\n",
            "[208]\n",
            "[7]\n",
            "[3]\n",
            "[201]\n",
            "[8]\n",
            "[17]\n",
            "[784]\n",
            "[3]\n",
            "[418]\n",
            "[287]\n",
            "[23]\n",
            "[2]\n",
            "[8]\n",
            "[9]\n",
            "[809]\n",
            "[179]\n",
            "[16]\n",
            "[227]\n",
            "[7]\n",
            "[3]\n",
            "[3678]\n",
            "[7]\n",
            "[1]\n",
            "[3679]\n",
            "[1]\n",
            "[65]\n",
            "[74]\n",
            "[15]\n",
            "[21]\n",
            "[2]\n",
            "[473]\n",
            "[57]\n",
            "[415]\n",
            "[15]\n",
            "[47]\n",
            "[114]\n",
            "[5]\n",
            "[413]\n",
            "[5]\n",
            "\n",
            "Generated Sequence: there must have lasted indefinitely except for an overwound clock recovering himself in a minute he had often a policeman married him and he was leaning against her world in a lull in the entertainment the man looked at me and smiled back glanced at daisy want to speak to\n",
            "\n",
            "[948]\n",
            "[214]\n",
            "[9]\n",
            "[1060]\n",
            "[12]\n",
            "[177]\n",
            "[181]\n",
            "[186]\n",
            "[2085]\n",
            "[7]\n",
            "[1]\n",
            "[1313]\n",
            "[2788]\n",
            "[4]\n",
            "[1961]\n",
            "[1]\n",
            "[1700]\n",
            "[6]\n",
            "[129]\n",
            "[18]\n",
            "[28]\n",
            "[143]\n",
            "[24]\n",
            "[3]\n",
            "[545]\n",
            "[2496]\n",
            "[124]\n",
            "[2319]\n",
            "[34]\n",
            "[55]\n",
            "[57]\n",
            "[22]\n",
            "[8]\n",
            "[74]\n",
            "[15]\n",
            "[11]\n",
            "[592]\n",
            "[751]\n",
            "[30]\n",
            "[2]\n",
            "[16]\n",
            "[69]\n",
            "[343]\n",
            "[168]\n",
            "[39]\n",
            "[73]\n",
            "[1480]\n",
            "[2467]\n",
            "[5]\n",
            "[2581]\n",
            "\n",
            "Generated Sequence: damned every was sorry it three baker sat discreetly in the tragic griefs of speech the wedding i began on all girl from a trembling match where perspiration gatsby came back as he looked at his watch jumped up and her eyes fell upon tom not perfectly willing to indicate\n",
            "\n",
            "[14]\n",
            "[241]\n",
            "[417]\n",
            "[2067]\n",
            "[14]\n",
            "[317]\n",
            "[2146]\n",
            "[287]\n",
            "[23]\n",
            "[135]\n",
            "[6]\n",
            "[132]\n",
            "[6]\n",
            "[9]\n",
            "[3]\n",
            "[19]\n",
            "[26]\n",
            "[455]\n",
            "[132]\n",
            "[8]\n",
            "[9]\n",
            "[881]\n",
            "[41]\n",
            "[14]\n",
            "[3]\n",
            "[563]\n",
            "[4]\n",
            "[153]\n",
            "[3]\n",
            "[1678]\n",
            "[4]\n",
            "[11]\n",
            "[4751]\n",
            "[87]\n",
            "[146]\n",
            "[51]\n",
            "[6]\n",
            "[45]\n",
            "[3]\n",
            "[398]\n",
            "[8]\n",
            "[17]\n",
            "[56]\n",
            "[950]\n",
            "[4]\n",
            "[182]\n",
            "[589]\n",
            "[5]\n",
            "[34]\n",
            "[1030]\n",
            "\n",
            "Generated Sequence: you demanded catherine forced you myrtle considered married him because i thought i was a she said finally thought he was wrong about you a lot of can a glimpse of his antecedents which even so i have a least he had been lying of next occurred to gatsby touching\n",
            "\n",
            "[619]\n",
            "[2465]\n",
            "[717]\n",
            "[100]\n",
            "[3]\n",
            "[538]\n",
            "[1134]\n",
            "[20]\n",
            "[1]\n",
            "[193]\n",
            "[361]\n",
            "[9]\n",
            "[31]\n",
            "[3304]\n",
            "[1007]\n",
            "[831]\n",
            "[476]\n",
            "[183]\n",
            "[2668]\n",
            "[1]\n",
            "[361]\n",
            "[124]\n",
            "[32]\n",
            "[140]\n",
            "[3]\n",
            "[3860]\n",
            "[345]\n",
            "[7]\n",
            "[116]\n",
            "[777]\n",
            "[38]\n",
            "[35]\n",
            "[339]\n",
            "[15]\n",
            "[531]\n",
            "[279]\n",
            "[4]\n",
            "[1]\n",
            "[491]\n",
            "[173]\n",
            "[15]\n",
            "[174]\n",
            "[77]\n",
            "[2]\n",
            "[6]\n",
            "[203]\n",
            "[21]\n",
            "[253]\n",
            "[329]\n",
            "[37]\n",
            "\n",
            "Generated Sequence: real massed leaves made a cool places for the front porch was an overenlarged photograph apparently including most crossing the porch where we heard a haunting friend in their flowers there were sitting at either end of the couch looking at each other and i called me home business into\n",
            "\n",
            "[3]\n",
            "[352]\n",
            "[7]\n",
            "[1]\n",
            "[1679]\n",
            "[5050]\n",
            "[11]\n",
            "[5042]\n",
            "[2]\n",
            "[2586]\n",
            "[5043]\n",
            "[15]\n",
            "[1]\n",
            "[68]\n",
            "[219]\n",
            "[2]\n",
            "[43]\n",
            "[32]\n",
            "[35]\n",
            "[339]\n",
            "[53]\n",
            "[3]\n",
            "[1143]\n",
            "[4]\n",
            "[219]\n",
            "[22]\n",
            "[40]\n",
            "[1]\n",
            "[267]\n",
            "[9]\n",
            "[1548]\n",
            "[9]\n",
            "[360]\n",
            "[18]\n",
            "[1]\n",
            "[188]\n",
            "[53]\n",
            "[147]\n",
            "[24]\n",
            "[1]\n",
            "[2978]\n",
            "[2979]\n",
            "[46]\n",
            "[17]\n",
            "[56]\n",
            "[1915]\n",
            "[5]\n",
            "[33]\n",
            "[58]\n",
            "[143]\n",
            "\n",
            "Generated Sequence: a year in the pump shading his establishment and gazed holloweyed at the house life and when we were sitting down a string of life as if the money was broadway was standing on the table down off from the anchored balloon they had been football to this little girl\n",
            "\n",
            "[2057]\n",
            "[37]\n",
            "[1456]\n",
            "[3]\n",
            "[2141]\n",
            "[2]\n",
            "[1584]\n",
            "[4043]\n",
            "[11]\n",
            "[101]\n",
            "[46]\n",
            "[667]\n",
            "[2365]\n",
            "[407]\n",
            "[4986]\n",
            "[324]\n",
            "[2]\n",
            "[3]\n",
            "[229]\n",
            "[2535]\n",
            "[2306]\n",
            "[2]\n",
            "[1]\n",
            "[1925]\n",
            "[2091]\n",
            "[2664]\n",
            "[26]\n",
            "[13]\n",
            "[16]\n",
            "[573]\n",
            "[169]\n",
            "[4416]\n",
            "[9]\n",
            "[432]\n",
            "[332]\n",
            "[2]\n",
            "[50]\n",
            "[1]\n",
            "[1699]\n",
            "[2]\n",
            "[3]\n",
            "[65]\n",
            "[10]\n",
            "[5]\n",
            "[190]\n",
            "[4927]\n",
            "[2099]\n",
            "[20]\n",
            "[3]\n",
            "[134]\n",
            "\n",
            "Generated Sequence: wheat into phase a catholic and lemons haag his hand they pushed port short sail person and a rather impulse witness and the burning swung closest said with her breath without happenings was deep summer and then the officer and a man that to nothing perspired delicately for a while\n",
            "\n",
            "[1]\n",
            "[1274]\n",
            "[65]\n",
            "[2475]\n",
            "[37]\n",
            "[877]\n",
            "[447]\n",
            "[5]\n",
            "[21]\n",
            "[60]\n",
            "[9]\n",
            "[493]\n",
            "[6]\n",
            "[9]\n",
            "[545]\n",
            "[507]\n",
            "[6]\n",
            "[17]\n",
            "[75]\n",
            "[2542]\n",
            "[4]\n",
            "[373]\n",
            "[5065]\n",
            "[2]\n",
            "[48]\n",
            "[973]\n",
            "[67]\n",
            "[34]\n",
            "[13]\n",
            "[5322]\n",
            "[5323]\n",
            "[689]\n",
            "[238]\n",
            "[8]\n",
            "[156]\n",
            "[36]\n",
            "[47]\n",
            "[226]\n",
            "[144]\n",
            "[1]\n",
            "[845]\n",
            "[9]\n",
            "[38]\n",
            "[3]\n",
            "[494]\n",
            "[3668]\n",
            "[1422]\n",
            "[2]\n",
            "[510]\n",
            "[2]\n",
            "\n",
            "Generated Sequence: the owleyed man break into ghostly laughter to me who was beyond i was trembling late i had no consciousness of being observed and one emotion after gatsby with indiscernible barbed wire between he found or daisy our right the apartment was there a silver tinny game and voices and\n",
            "\n",
            "[28]\n",
            "[241]\n",
            "[3]\n",
            "[530]\n",
            "[649]\n",
            "[89]\n",
            "[16]\n",
            "[5086]\n",
            "[297]\n",
            "[13]\n",
            "[223]\n",
            "[5223]\n",
            "[332]\n",
            "[6]\n",
            "[140]\n",
            "[31]\n",
            "[247]\n",
            "[4]\n",
            "[4439]\n",
            "[374]\n",
            "[10]\n",
            "[3610]\n",
            "[36]\n",
            "[88]\n",
            "[69]\n",
            "[1582]\n",
            "[2]\n",
            "[194]\n",
            "[153]\n",
            "[1042]\n",
            "[4980]\n",
            "[623]\n",
            "[1353]\n",
            "[534]\n",
            "[6]\n",
            "[140]\n",
            "[1]\n",
            "[561]\n",
            "[595]\n",
            "[30]\n",
            "[342]\n",
            "[27]\n",
            "[38]\n",
            "[9]\n",
            "[3]\n",
            "[204]\n",
            "[2352]\n",
            "[2]\n",
            "[34]\n",
            "[6]\n",
            "\n",
            "Generated Sequence: all demanded a nice thin moment her wit rose with these wornout summer i heard an sort of choking matter that sooner or new eyes wagon and afternoon can lavender greedy whose flushed lips i heard the phone taken up inside but there was a great relief and gatsby i\n",
            "\n",
            "[19]\n",
            "[74]\n",
            "[15]\n",
            "[21]\n",
            "[1359]\n",
            "[22]\n",
            "[40]\n",
            "[8]\n",
            "[1626]\n",
            "[5429]\n",
            "[33]\n",
            "[439]\n",
            "[32]\n",
            "[344]\n",
            "[162]\n",
            "[2]\n",
            "[470]\n",
            "[268]\n",
            "[11]\n",
            "[251]\n",
            "[199]\n",
            "[22]\n",
            "[6]\n",
            "[35]\n",
            "[3]\n",
            "[138]\n",
            "[926]\n",
            "[87]\n",
            "[100]\n",
            "[1]\n",
            "[4553]\n",
            "[2]\n",
            "[50]\n",
            "[758]\n",
            "[94]\n",
            "[1]\n",
            "[121]\n",
            "[1]\n",
            "[182]\n",
            "[149]\n",
            "[9]\n",
            "[4920]\n",
            "[265]\n",
            "[1]\n",
            "[288]\n",
            "[7]\n",
            "[260]\n",
            "[941]\n",
            "[3]\n",
            "[2380]\n",
            "\n",
            "Generated Sequence: she looked at me anxiously as if he hoped corroborate this suppose we shook hands and continued among his feet still as i were a young affair which made the brush and then disappeared around the room the next day was broiling almost the girls in yellow glowing a tiny\n",
            "\n",
            "[10]\n",
            "[8]\n",
            "[854]\n",
            "[82]\n",
            "[3715]\n",
            "[22]\n",
            "[1]\n",
            "[3716]\n",
            "[2227]\n",
            "[1279]\n",
            "[43]\n",
            "[1]\n",
            "[805]\n",
            "[4]\n",
            "[1]\n",
            "[9]\n",
            "[48]\n",
            "[172]\n",
            "[4986]\n",
            "[4987]\n",
            "[311]\n",
            "[133]\n",
            "[1]\n",
            "[4988]\n",
            "[813]\n",
            "[69]\n",
            "[391]\n",
            "[11]\n",
            "[679]\n",
            "[1]\n",
            "[845]\n",
            "[4134]\n",
            "[1156]\n",
            "[13]\n",
            "[3]\n",
            "[3486]\n",
            "[15]\n",
            "[398]\n",
            "[163]\n",
            "[3]\n",
            "[3487]\n",
            "[3]\n",
            "[3488]\n",
            "[4]\n",
            "[3489]\n",
            "[55]\n",
            "[53]\n",
            "[13]\n",
            "[349]\n",
            "[411]\n",
            "\n",
            "Generated Sequence: that he grew more correct as the fraternal hilarity increased when the history of the was one small sail crawled slowly toward the fresher sea eyes followed his taxi the apartment comprehended blown with a thumb at least once a fortnight a corps of caterers came down with several hundred\n",
            "\n"
          ]
        }
      ]
    }
  ]
}